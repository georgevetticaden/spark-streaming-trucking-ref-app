{"paragraphs":[{"text":"// IOT Trucking Ref App Implemented using HDP 3.0 / Spark 2.3 Structured Streaming","user":"admin","dateUpdated":"2018-06-07T05:48:34+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":20,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1528350514982_-2132122939","id":"20180327-210615_629762194","dateCreated":"2018-06-07T05:48:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:9050"},{"title":"Setup Kafka Config Variables","text":"val kafkaBootStrapServers = \"connected-dp4.field.hortonworks.com:6667,connected-dp5.field.hortonworks.com:6667,connected-dp6.field.hortonworks.com:6667,connected-dp7.field.hortonworks.com:6667\"\nval geoStreamKafkaTopic   = \"truck_events_json\"\nval speedStreamKafkaTopic = \"truck_speed_events_json\"\nval speedingDriverAlertTopic = \"speeding_driver_alert_json\"\nval checkpointLocationInHDFS = \"/user/zeppelin/app/truck9\"","user":"admin","dateUpdated":"2018-06-07T05:48:34+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"kafkaBootStrapServers: String = connected-dp4.field.hortonworks.com:6667,connected-dp5.field.hortonworks.com:6667,connected-dp6.field.hortonworks.com:6667,connected-dp7.field.hortonworks.com:6667\ngeoStreamKafkaTopic: String = truck_events_json\nspeedStreamKafkaTopic: String = truck_speed_events_json\nspeedingDriverAlertTopic: String = speeding_driver_alert_json\ncheckpointLocationInHDFS: String = /user/zeppelin/app/truck9\n"}]},"apps":[],"jobName":"paragraph_1528350514985_-1131092171","id":"20180327-190544_1592332448","dateCreated":"2018-06-07T05:48:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9051"},{"title":"Create Schemas for the Geo and Speed Stream","text":"import org.apache.spark.sql.functions._\nimport org.apache.spark.sql.streaming._\nimport org.apache.spark.sql.types._\n\nval geoStreamSchema = new StructType()\n    .add(\"eventTime\", TimestampType)\n    .add(\"eventTimeLong\", LongType)\n    .add(\"eventSource\", StringType)\n    .add(\"truckId\", IntegerType)\n    .add(\"driverId\", IntegerType)\n    .add(\"driverName\", StringType)\n    .add(\"routeId\", IntegerType)\n    .add(\"route\", StringType)\n    .add(\"eventType\", StringType)\n    .add(\"latitude\", DoubleType)\n    .add(\"longitude\", DoubleType)\n    .add(\"correlationId\", LongType)\n    .add(\"geoAddress\", StringType)\n\n\nval speedStreamSchema = new StructType()\n    .add(\"eventTime\", TimestampType)\n    .add(\"eventTimeLong\", LongType)\n    .add(\"eventSource\", StringType)\n    .add(\"truckId\", IntegerType)\n    .add(\"driverId\", IntegerType)\n    .add(\"driverName\", StringType)\n    .add(\"routeId\", IntegerType)\n    .add(\"route\", StringType)\n    .add(\"speed\", IntegerType)\n\n","user":"admin","dateUpdated":"2018-06-07T05:48:34+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.functions._\nimport org.apache.spark.sql.streaming._\nimport org.apache.spark.sql.types._\ngeoStreamSchema: org.apache.spark.sql.types.StructType = StructType(StructField(eventTime,TimestampType,true), StructField(eventTimeLong,LongType,true), StructField(eventSource,StringType,true), StructField(truckId,IntegerType,true), StructField(driverId,IntegerType,true), StructField(driverName,StringType,true), StructField(routeId,IntegerType,true), StructField(route,StringType,true), StructField(eventType,StringType,true), StructField(latitude,DoubleType,true), StructField(longitude,DoubleType,true), StructField(correlationId,LongType,true), StructField(geoAddress,StringType,true))\nspeedStreamSchema: org.apache.spark.sql.types.StructType = StructType(StructField(eventTime,TimestampType,true), StructField(eventTimeLong,LongType,true), StructField(eventSource,StringType,true), StructField(truckId,IntegerType,true), StructField(driverId,IntegerType,true), StructField(driverName,StringType,true), StructField(routeId,IntegerType,true), StructField(route,StringType,true), StructField(speed,IntegerType,true))\n"}]},"apps":[],"jobName":"paragraph_1528350514986_1928905325","id":"20180327-190923_1211561570","dateCreated":"2018-06-07T05:48:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9052"},{"title":"Create a Streaming DataFrame that Reads events from Geo Stream Kafka Topic","text":"val geoStreamDataFrameRaw = spark\n  .readStream\n  .format(\"kafka\")\n  .option(\"kafka.bootstrap.servers\", kafkaBootStrapServers)\n  .option(\"subscribe\", geoStreamKafkaTopic )\n  .option(\"startingOffsets\", \"latest\")\n  .load()\n  \nval geoStreamEvents = geoStreamDataFrameRaw.select(from_json(col(\"value\").cast(\"string\"), geoStreamSchema).alias(\"geoStream\"))","user":"admin","dateUpdated":"2018-06-07T05:48:34+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"geoStreamDataFrameRaw: org.apache.spark.sql.DataFrame = [key: binary, value: binary ... 5 more fields]\ngeoStreamEvents: org.apache.spark.sql.DataFrame = [geoStream: struct<eventTime: timestamp, eventTimeLong: bigint ... 11 more fields>]\n"}]},"apps":[],"jobName":"paragraph_1528350514987_906752577","id":"20180327-191942_2076965265","dateCreated":"2018-06-07T05:48:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9053"},{"title":"Create a Streaming DataFrame that Reads events from Speed Stream Kafka Topic","text":"val speedStreamDataFrameRaw = spark\n  .readStream\n  .format(\"kafka\")\n  .option(\"kafka.bootstrap.servers\", kafkaBootStrapServers)\n  .option(\"subscribe\", speedStreamKafkaTopic )\n  .option(\"startingOffsets\", \"latest\")\n  .load()\n  \nval speedStreamEvents = speedStreamDataFrameRaw.select(from_json(col(\"value\").cast(\"string\"), speedStreamSchema).alias(\"speedStream\"))","user":"admin","dateUpdated":"2018-06-07T05:48:34+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"speedStreamDataFrameRaw: org.apache.spark.sql.DataFrame = [key: binary, value: binary ... 5 more fields]\nspeedStreamEvents: org.apache.spark.sql.DataFrame = [speedStream: struct<eventTime: timestamp, eventTimeLong: bigint ... 7 more fields>]\n"}]},"apps":[],"jobName":"paragraph_1528350514988_799228958","id":"20180328-044006_1046246995","dateCreated":"2018-06-07T05:48:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9054"},{"title":"Join the Geo and Speed Streams with Watermarking","text":"//Configure watermarks for the stream so that we can age out the join buffers over time. This configures the maximum bounds for late data\n\nval geoStreamWithWatermark = geoStreamEvents.selectExpr(\"geoStream.*\").withWatermark(\"eventTime\", \"10 seconds\")\n                                            \n\nval speedStreamWithWatermark = speedStreamEvents.selectExpr(\"speedStream.eventTime as s_eventTime\",\n                                                            \"speedStream.driverId as s_driverId\",\n                                                            \"speedStream.speed as speed\"\n                                                            ).withWatermark(\"s_eventTime\", \"15 seconds\")\n                                                \n\n//join the streams\nval joinedGeoAndSpeedStream = geoStreamWithWatermark.join(speedStreamWithWatermark,\n                                                         expr(\"\"\" \n                                                            driverId == s_driverId AND \n                                                            s_eventTime between eventTime AND eventTime + interval 2 second \"\"\"))\n                                                    .drop(\"s_eventTime\", \"s_driverId\")\n                                              \njoinedGeoAndSpeedStream.printSchema()\n","user":"admin","dateUpdated":"2018-06-07T05:48:34+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"geoStreamWithWatermark: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [eventTime: timestamp, eventTimeLong: bigint ... 11 more fields]\nspeedStreamWithWatermark: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [s_eventTime: timestamp, s_driverId: int ... 1 more field]\njoinedGeoAndSpeedStream: org.apache.spark.sql.DataFrame = [eventTime: timestamp, eventTimeLong: bigint ... 12 more fields]\nroot\n |-- eventTime: timestamp (nullable = true)\n |-- eventTimeLong: long (nullable = true)\n |-- eventSource: string (nullable = true)\n |-- truckId: integer (nullable = true)\n |-- driverId: integer (nullable = true)\n |-- driverName: string (nullable = true)\n |-- routeId: integer (nullable = true)\n |-- route: string (nullable = true)\n |-- eventType: string (nullable = true)\n |-- latitude: double (nullable = true)\n |-- longitude: double (nullable = true)\n |-- correlationId: long (nullable = true)\n |-- geoAddress: string (nullable = true)\n |-- speed: integer (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1528350514988_-1301338983","id":"20180404-163302_1775690879","dateCreated":"2018-06-07T05:48:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9055"},{"title":"Filter on Violation Events","text":"val filteredViolationEventsStream = joinedGeoAndSpeedStream.filter(\"eventType != 'Normal'\")","user":"admin","dateUpdated":"2018-06-07T05:48:34+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"filteredViolationEventsStream: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [eventTime: timestamp, eventTimeLong: bigint ... 12 more fields]\n"}]},"apps":[],"jobName":"paragraph_1528350514989_1462426056","id":"20180405-172635_440814196","dateCreated":"2018-06-07T05:48:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9056"},{"title":"Calculate Driver Avg Speed over 3 Minute Window","text":"\n//Define Window Calculation\nval tumblingWindowAverageDriverSpeedStream = filteredViolationEventsStream.withWatermark(\"eventTime\", \"3 minutes\").\n                                                                           groupBy(col(\"driverId\"), col(\"driverName\"), col(\"route\"), \n                                                                                        window(col(\"eventTime\"), \"3 minutes\"))\n                                                                                  .agg(avg(\"speed\").as(\"speed_AVG\"))","user":"admin","dateUpdated":"2018-06-07T05:48:34+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"tumblingWindowAverageDriverSpeedStream: org.apache.spark.sql.DataFrame = [driverId: int, driverName: string ... 3 more fields]\n"}]},"apps":[],"jobName":"paragraph_1528350514990_-1129160249","id":"20180328-052944_1668052708","dateCreated":"2018-06-07T05:48:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9057"},{"title":"Filter - Speeding Drivers ","text":"val speedingDriversStream = tumblingWindowAverageDriverSpeedStream.filter(\"speed_AVG > 80\")\nspeedingDriversStream.printSchema","user":"admin","dateUpdated":"2018-06-07T05:48:34+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"speedingDriversStream: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [driverId: int, driverName: string ... 3 more fields]\nroot\n |-- driverId: integer (nullable = true)\n |-- driverName: string (nullable = true)\n |-- route: string (nullable = true)\n |-- window: struct (nullable = false)\n |    |-- start: timestamp (nullable = true)\n |    |-- end: timestamp (nullable = true)\n |-- speed_AVG: double (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1528350514991_-2058582190","id":"20180405-211449_1929025426","dateCreated":"2018-06-07T05:48:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9058"},{"title":"Start the Stream Application and Write Speeding Alerts to Kafka Topic","text":"val speedingAlertsStreamingApp = speedingDriversStream\n    .selectExpr(\"CAST(window.start AS STRING) AS key\", \"to_json(struct(*)) AS value\")\n    .writeStream\n    .format(\"kafka\")\n    .option(\"kafka.bootstrap.servers\", kafkaBootStrapServers)\n    .option(\"kafka.client.id\", \"spark-speeding-alert-producer\")\n    .option(\"topic\", speedingDriverAlertTopic)\n    .option(\"checkpointLocation\", checkpointLocationInHDFS)\n    .queryName(\"speedingDriversTable\")\n    .start()","user":"admin","dateUpdated":"2018-06-07T05:48:34+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"java.lang.IllegalArgumentException: Cannot start query with name speedingDriversTable as a query with that name is already active\n  at org.apache.spark.sql.streaming.StreamingQueryManager$$anonfun$startQuery$1.apply(StreamingQueryManager.scala:315)\n  at org.apache.spark.sql.streaming.StreamingQueryManager$$anonfun$startQuery$1.apply(StreamingQueryManager.scala:313)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:313)\n  at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:296)\n  ... 59 elided\n"}]},"apps":[],"jobName":"paragraph_1528350514992_-1658563789","id":"20180405-212041_2050526126","dateCreated":"2018-06-07T05:48:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9059"},{"title":"Stop the Sreaming Application","text":"//speedingAlertsStreamingApp.stop","user":"admin","dateUpdated":"2018-06-07T05:48:34+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1528350514992_104201220","id":"20180328-044433_1881348926","dateCreated":"2018-06-07T05:48:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9060"},{"user":"admin","dateUpdated":"2018-06-07T05:48:34+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1528350514993_2008997075","id":"20180423-174855_153893244","dateCreated":"2018-06-07T05:48:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9061"},{"user":"admin","dateUpdated":"2018-06-07T05:48:34+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1528350514994_1284063053","id":"20180409-190205_1368245303","dateCreated":"2018-06-07T05:48:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9062"}],"name":"Trucking Ref App Streaming Analytics","id":"2DFW9NRGY","noteParams":{},"noteForms":{},"angularObjects":{"spark2:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}